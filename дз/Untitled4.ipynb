{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03eebba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# use Performer, as it had the best reported numbers\n",
    "\n",
    "from performer_pytorch import SelfAttention as PerformerAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e2abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def get_module_device(module):\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "def find_modules(nn_module, type):\n",
    "    return [module for module in nn_module.modules() if isinstance(module, type)]\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        causal = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads =  heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.causal = causal\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        b, n, d, h, device = *x.shape, self.heads, x.device\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        max_neg_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b i -> b i ()') * rearrange(mask, 'b j -> b () j')\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = sim.shape[-2:]\n",
    "            causal_mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n",
    "            causal_mask = rearrange(causal_mask, 'i j -> () i j')\n",
    "            sim.masked_fill_(causal_mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078c09a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main class\n",
    "\n",
    "class Omninet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        pool_layer_tokens_every = 2,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        feature_redraw_interval = 1000\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = nn.ModuleList([])\n",
    "        for ind in range(depth):\n",
    "            num_layers = ind + 1\n",
    "            should_pool = num_layers % pool_layer_tokens_every\n",
    "\n",
    "            layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)),\n",
    "                PreNorm(dim, FeedForward(dim = dim, dropout = ff_dropout)),\n",
    "                PerformerAttention(dim = dim, heads= heads, dim_head = dim_head) if should_pool else None\n",
    "            ]))\n",
    "\n",
    "        self.layers = layers\n",
    "        self.pool_num_layers = pool_layer_tokens_every\n",
    "\n",
    "        # keep track of redrawing projection matrix for Performer\n",
    "        self.feature_redraw_interval = feature_redraw_interval\n",
    "        self.register_buffer('calls_since_last_redraw', torch.tensor(0))\n",
    "\n",
    "    def fix_projection_matrices_(self):\n",
    "        self.feature_redraw_interval = None\n",
    "\n",
    "    def check_redraw_projections(self):\n",
    "        if not self.training:\n",
    "            return\n",
    "\n",
    "        if exists(self.feature_redraw_interval) and self.calls_since_last_redraw >= self.feature_redraw_interval:\n",
    "            device = get_module_device(self)\n",
    "\n",
    "            fast_attentions = find_modules(self, FastAttention)\n",
    "            for fast_attention in fast_attentions:\n",
    "                fast_attention.redraw_projection_matrix(device)\n",
    "\n",
    "            self.calls_since_last_redraw.zero_()\n",
    "            return\n",
    "\n",
    "        self.calls_since_last_redraw += 1\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        self.check_redraw_projections()\n",
    "        pool_num_layers = self.pool_num_layers\n",
    "\n",
    "        hiddens = [x]\n",
    "\n",
    "        for attn, ff, efficient_attn in self.layers:\n",
    "            x = attn(x, mask = mask) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "            hiddens.append(x)\n",
    "            if exists(efficient_attn):\n",
    "                layers_to_pool = hiddens[-pool_num_layers:]\n",
    "                num_layers = len(layers_to_pool)\n",
    "\n",
    "                all_tokens = torch.stack(layers_to_pool)\n",
    "                all_tokens = rearrange(all_tokens, 'l b n d -> b (n l) d')\n",
    "\n",
    "                pool_attn_mask = None\n",
    "                if exists(mask):\n",
    "                    pool_attn_mask = repeat(mask, 'b n -> b (n l)', l = num_layers)\n",
    "\n",
    "                attended_tokens = efficient_attn(all_tokens, mask = pool_attn_mask)\n",
    "\n",
    "                attended_tokens = rearrange(attended_tokens, 'b n c -> b c n')\n",
    "                pooled_tokens = F.max_pool1d(attended_tokens, kernel_size = num_layers, stride = num_layers)\n",
    "                x += rearrange(pooled_tokens, 'b c n -> b n c')\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f93c18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# causal case is sufficiently different to warrant its own class\n",
    "# use layer axial attention for now, until I rewrite the linear attention cuda kernel\n",
    "\n",
    "class OmninetCausal(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        pool_layer_tokens_every = 2,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_pos_emb = nn.Parameter(torch.randn(depth + 1, dim))\n",
    "\n",
    "        layers = nn.ModuleList([])\n",
    "        for ind in range(depth):\n",
    "            num_layers = ind + 1\n",
    "            should_pool = num_layers % pool_layer_tokens_every\n",
    "\n",
    "            layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(causal = True, dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)),\n",
    "                PreNorm(dim, FeedForward(dim = dim, dropout = ff_dropout)),\n",
    "                Attention(dim = dim, heads= heads, dim_head = dim_head) if should_pool else None\n",
    "            ]))\n",
    "\n",
    "        self.layers = layers\n",
    "        self.pool_num_layers = pool_layer_tokens_every\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        pool_num_layers = self.pool_num_layers\n",
    "\n",
    "        b = x.shape[0]\n",
    "        pos_embs = rearrange(self.layer_pos_emb, 'n d -> () n d')\n",
    "\n",
    "        x += pos_embs[:, 0]\n",
    "        hiddens = [x]\n",
    "\n",
    "        for ind, (attn, ff, layer_axial_attn) in enumerate(self.layers):\n",
    "\n",
    "            x = attn(x, mask = mask) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "            x += pos_embs[:, ind + 1]\n",
    "            hiddens.append(x)\n",
    "\n",
    "            if exists(layer_axial_attn):\n",
    "                layers_to_pool = hiddens[-pool_num_layers:]\n",
    "                num_layers = len(layers_to_pool)\n",
    "\n",
    "                layer_tokens = rearrange(torch.stack(layers_to_pool), 'l b n d -> (b n) l d')\n",
    "\n",
    "                attended_tokens = layer_axial_attn(layer_tokens)\n",
    "                attended_tokens = rearrange(attended_tokens, '(b n) l d -> b n l d', b = b)\n",
    "                pooled_attended_tokens = attended_tokens.max(dim = -2).values\n",
    "                x += pooled_attended_tokens\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "960bff91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9816,  1.4537,  2.9661,  ...,  0.2320, -0.9211, -0.4982],\n",
       "         [-1.1163,  0.2141,  0.1512,  ...,  1.9984,  0.1828, -0.5664],\n",
       "         [ 0.9360, -0.6398, -0.1149,  ...,  0.2647, -0.9080,  0.2136],\n",
       "         ...,\n",
       "         [-0.9472,  0.6716,  1.9491,  ...,  0.5262, -0.3791, -0.6060],\n",
       "         [ 3.3216,  0.5934,  2.6454,  ...,  0.7024, -0.3119, -1.1473],\n",
       "         [ 0.0977, -0.9421,  1.7860,  ...,  2.1599,  1.3780, -1.1123]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omninet = Omninet(\n",
    "    dim = 512,                     # model dimension\n",
    "    depth = 6,                     # depth\n",
    "    dim_head = 64,                 # dimension per head\n",
    "    heads = 8,                     # number of heads\n",
    "    pool_layer_tokens_every = 3,   # key to this paper - every N layers, omni attend to all tokens of all layers\n",
    "    attn_dropout = 0.1,            # attention dropout\n",
    "    ff_dropout = 0.1,              # feedforward dropout\n",
    "    feature_redraw_interval = 1000 # how often to redraw the projection matrix for omni attention net - Performer\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 1024, 512)\n",
    "mask = torch.ones(1, 1024).bool()\n",
    "\n",
    "omninet(x, mask = mask) # (1, 1024, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b23e4b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.5471,  6.6507,  4.3266,  ...,  4.1534,  0.9080,  3.8440],\n",
       "         [ 6.0989,  4.7633,  4.2315,  ...,  2.0625,  0.6029,  1.1094],\n",
       "         [ 6.5093,  5.0774,  3.3120,  ...,  5.8083,  0.0604,  2.0568],\n",
       "         ...,\n",
       "         [ 4.2883,  5.6745,  4.2724,  ...,  4.3588,  4.1432,  3.7385],\n",
       "         [ 5.8884,  4.6082,  4.3106,  ...,  3.4019,  2.5031,  1.3954],\n",
       "         [ 5.8154,  4.0144,  2.9318,  ...,  5.6129,  1.8431, -0.3573]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omninet = OmninetCausal(\n",
    "    dim = 512,                     # model dimension\n",
    "    depth = 6,                     # depth\n",
    "    dim_head = 64,                 # dimension per head\n",
    "    heads = 8,                     # number of heads\n",
    "    pool_layer_tokens_every = 3,   # key to this paper - every N layers, omni attend to all tokens of all layers\n",
    "    attn_dropout = 0.1,            # attention dropout\n",
    "    ff_dropout = 0.1               # feedforward dropout\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 1024, 512)\n",
    "mask = torch.ones(1, 1024).bool()\n",
    "\n",
    "omninet(x, mask = mask) # (1, 1024, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c77ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
